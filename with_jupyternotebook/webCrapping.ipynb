{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316e3fc8",
   "metadata": {},
   "source": [
    "# Test Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654afb2",
   "metadata": {},
   "source": [
    "<h3>Importing packages<h3>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43caee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89771c",
   "metadata": {},
   "source": [
    "<h3>Tracking performance. Tracking time to scrap each page<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b55f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "page_counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b3dbc",
   "metadata": {},
   "source": [
    "<h3>Creating empty lists to store data in<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d30daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = []\n",
    "price_list = []\n",
    "stock_list = []\n",
    "star_list = []\n",
    "num_available = []\n",
    "category_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8bc3e",
   "metadata": {},
   "source": [
    "<h3>Requesting pages to scrap<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d01d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text = requests.get('http://books.toscrape.com/catalogue/page-1.html').text\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "button = soup.find('li', class_='next')\n",
    "pager=button.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e336f",
   "metadata": {},
   "source": [
    "<h3>Wrapping the code in  a function<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2a892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_books():\n",
    "    books = soup.find_all('li', class_='col-xs-6 col-sm-4 col-md-3 col-lg-3')    \n",
    "    for book in books:\n",
    "        book_title = book.article.h3.a['title']\n",
    "        book_price_pound = book.article.find('div', class_='product_price').p.text\n",
    "        book_price =re.findall('Â£([0-9.0]+)', book_price_pound) \n",
    "        avail=book.article.find('div', class_='product_price').find('p', class_='instock availability').text.strip()\n",
    "        stars = book.article.p['class']\n",
    "        star = stars[1]\n",
    "        \n",
    "        #Storing data in empty lists\n",
    "        book_list.append(book_title)\n",
    "        price_list.append(float(book_price[0]))\n",
    "        stock_list.append(avail)\n",
    "        star_list.append(star)\n",
    "        \n",
    "        #Checking quantity of books available\n",
    "        book_link = requests.get('http://books.toscrape.com/catalogue/'+book.article.h3.a['href']).text\n",
    "        url = book_link\n",
    "        soup2 = BeautifulSoup(url, 'lxml')\n",
    "        stock_avail = soup2.find('p', class_='instock availability').text.strip()\n",
    "        \n",
    "        #Using regular expression to isolate numerical values\n",
    "        stock_nums = re.findall('[0-9]+', stock_avail)\n",
    "        stock_num = stock_nums[0]\n",
    "        num_available.append(stock_num)\n",
    "        \n",
    "        #Scraping book catergory\n",
    "        cnt=0\n",
    "        category = soup2.find_all('li')\n",
    "        for i in category:\n",
    "            cnt+=1\n",
    "            #Book's category is number 3 on the list\n",
    "            if cnt ==3:\n",
    "                category_list.append(i.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee83e8f",
   "metadata": {},
   "source": [
    "<h3>A function to run the program as long as there in 'next' page to scrap<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9eea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page #:  1\n",
      "Minutes:  4.865206849575043\n",
      "Page #:  2\n",
      "Minutes:  5.080256946881613\n",
      "Page #:  3\n",
      "Minutes:  5.29583603143692\n",
      "Page #:  4\n",
      "Minutes:  5.524510331948599\n",
      "Page #:  5\n",
      "Minutes:  5.745488500595092\n",
      "Page #:  6\n",
      "Minutes:  5.962447468439738\n",
      "Page #:  7\n",
      "Minutes:  6.180831968784332\n",
      "Page #:  8\n",
      "Minutes:  6.417035349210104\n",
      "Page #:  9\n",
      "Minutes:  6.631955254077911\n",
      "Page #:  10\n",
      "Minutes:  6.855706652005513\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    while pager == 'next':\n",
    "        #Tracking Performance\n",
    "        page_counter+=1\n",
    "        end_time = time.time()\n",
    "        seconds = end_time - start_time\n",
    "        minutes = seconds/60\n",
    "        print('Page #: ', page_counter)\n",
    "        print('Minutes: ', minutes)\n",
    "        \n",
    "        \n",
    "        #Requesting multiple pages if available. If no more pages, break\n",
    "        try:\n",
    "            find_books()\n",
    "            button = soup.find('li', class_='next')\n",
    "            next_page = button.a['href']\n",
    "            url = 'http://books.toscrape.com/catalogue/'+str(next_page)\n",
    "            pager=button.text\n",
    "            html_text = requests.get(url).text\n",
    "            soup = BeautifulSoup(html_text, 'lxml')\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print('Done')\n",
    "            pager = 'last'\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1074b2",
   "metadata": {},
   "source": [
    "<h3>Merging lists to create a dataframe<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc184543",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(list(zip(book_list, category_list,star_list,price_list,stock_list,num_available)), columns = ['Title','Book_category','Star_rating', 'Price', 'Stock', 'Quantity'])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984443c",
   "metadata": {},
   "source": [
    "<h3>Saving my Dataset to csv<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1691bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('Link to save a file .csv',sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
